[net]
channels=3
height=320
width=320


# layer 0: 64
[focus]

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=2
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=2
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=2
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=2
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-28
separation=rear

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-10

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-15

[route]
layers=-45
separation=rear

[route]
layers=-1,-2

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# layer 1: 64 -> 128
[focus]

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-28
separation=rear

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-10

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-15

[route]
layers=-45
separation=rear

[route]
layers=-1,-2

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# layer 2: 128 -> 256
[focus]

[convolution]
filters=128
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-52
separation=rear

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-16

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-21

[route]
layers=-74
separation=rear

[route]
layers=-1,-2

[convolution]
filters=128
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# layer 3: 256 -> 512
[focus]

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=32
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=32
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=32
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=32
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-28
separation=rear

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-10

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-15

[route]
layers=-45
separation=rear

[route]
layers=-1,-2

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# neck
[convolution]
filters=128
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=128
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=128
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# decode
# decode 0: 256
[upsample]
scale_factor=2

[route]
layers=-1,173

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=32
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=32
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=32
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=32
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-28
separation=rear

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-10

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-15

[route]
layers=-45
separation=rear

[route]
layers=-1,-2

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# decode 1
[upsample]
scale_factor=2

[route]
layers=-1,95

[convolution]
filters=128
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-28
separation=rear

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-10

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-15

[route]
layers=-45
separation=rear

[route]
layers=-1,-2

[convolution]
filters=128
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# decode 2
[upsample]
scale_factor=2

[route]
layers=-1,47

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-28
separation=rear

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-10

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-15

[route]
layers=-45
separation=rear

[route]
layers=-1,-2

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# decode 3
[upsample]
scale_factor=2

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=2
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=2
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=2
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=2
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-28
separation=rear

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-10

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-15

[route]
layers=-45
separation=rear

[route]
layers=-1,-2

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# conv set
[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=4
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=2
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=1
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[head]
