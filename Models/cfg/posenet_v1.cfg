[net]
channels=9
height=320
width=320


# layer 0: 64
[focus]

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=2
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=2
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=2
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=2
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=2
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=2
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=64
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-40
separation=rear

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-13

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-18

[route]
layers=-59
separation=rear

[route]
layers=-1,-2

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# layer 1: 64 -> 128
[focus]

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=4
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=4
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=128
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-40
separation=rear

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=8
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=8
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-13

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-18

[route]
layers=-59
separation=rear

[route]
layers=-1,-2

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# layer 2: 128 -> 256
[focus]

[convolution]
filters=128
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=8
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=8
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=256
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-112
separation=rear

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=16
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=16
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-31

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-36

[route]
layers=-149
separation=rear

[route]
layers=-1,-2

[convolution]
filters=128
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm


# layer 3: 256 -> 512
[focus]

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1
separation=front

[route]
layers=-1
separation=front

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# attention start
[attention]
heads=32
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=32
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=32
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=32
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

# attention start
[attention]
heads=32
window_size=10

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[attention]
heads=32
window_size=10
roll=1

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3

[channelMLP]
hidden_dim=512
activation=LeakyReLU

[normalization]
normalization=LayerNormTrans

[shortcut]
from=-3
# attention end

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-40
separation=rear

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# csp layer start
[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-2

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# res unit start
[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

# res unit start
[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=32
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[shortcut]
from=-3
# res unit end

[convolution]
filters=32
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm
# csp layer end

[route]
layers=-1,-13

[convolution]
filters=64
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[route]
layers=-1,-18

[route]
layers=-59
separation=rear

[route]
layers=-1,-2

[convolution]
filters=256
size=1
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

# conv set
[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=128
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=128
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[convolution]
filters=256
size=3
stride=1
pad=1
activation=LeakyReLU
normalization=BatchNorm

[head]
